{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Core_Original.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOuoOyxSqxnMzwjV9bP58me",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saikiran2711/resnet50_face_recognition/blob/main/Core_Original.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n65eLEL_Ge5"
      },
      "source": [
        "!pip install mtcnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji6ZOd5Y_OLb"
      },
      "source": [
        "!pip install keras_vggface"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtySa0HN_l4v"
      },
      "source": [
        "!pip install keras_applications"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bddEnIYNmPQ"
      },
      "source": [
        "import keras\n",
        "from matplotlib import pyplot\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "from scipy.spatial.distance import cosine\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "from keras_vggface.vggface import VGGFace\n",
        "from keras_vggface.utils import preprocess_input"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1vD1UeG8j7p"
      },
      "source": [
        "# face verification with the VGGFace2 model\n",
        "\n",
        "\n",
        "# extract a single face from a given photograph\n",
        "def extract_face(filename, required_size=(224, 224)):\n",
        "  # load image from file\n",
        "  pixels = pyplot.imread(filename)\n",
        "  # create the detector, using default weights\n",
        "  detector = MTCNN()\n",
        "  # detect faces in the image\n",
        "  results = detector.detect_faces(pixels)\n",
        "  # extract the bounding box from the first face\n",
        "  x1, y1, width, height = results[0]['box']\n",
        "  x2, y2 = x1 + width, y1 + height\n",
        "  # extract the face\n",
        "  face = pixels[y1:y2, x1:x2]\n",
        "  # resize pixels to the model size\n",
        "  image = Image.fromarray(face)\n",
        "  image = image.resize(required_size)\n",
        "  face_array = asarray(image)\n",
        "  display(Image.fromarray(face_array))\n",
        "  return face_array\n",
        "\n",
        "# extract faces and calculate face embeddings for a list of photo files\n",
        "def get_embeddings(filenames):\n",
        "  # extract faces\n",
        "  faces = [extract_face(f) for f in filenames]\n",
        "  # convert into an array of samples\n",
        "  samples = asarray(faces, 'float32')\n",
        "  # prepare the face for the model, e.g. center pixels\n",
        "\n",
        "  samples = preprocess_input(samples, version=2)\n",
        "  \n",
        "  # create a vggface model\n",
        "  model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
        "  # perform prediction\n",
        "  yhat = model.predict(samples)\n",
        "  return yhat\n",
        "\n",
        "# determine if a candidate face is a match for a known face\n",
        "def is_match(known_embedding, candidate_embedding, thresh=0.5):\n",
        "\t# calculate distance between embeddings\n",
        "\tscore = cosine(known_embedding, candidate_embedding)\n",
        "\tif score <= thresh:\n",
        "\t\tprint('>face is a Match (%.3f <= %.3f)' % (score, thresh))\n",
        "\telse:\n",
        "\t\tprint('>face is NOT a Match (%.3f > %.3f)' % (score, thresh))\n",
        "\n",
        "# define filenames\n",
        "filenames = ['/content/amirg.jpg','/content/photo.jpg']\n",
        "# get embeddings file filenames\n",
        "embeddings = get_embeddings(filenames)\n",
        "# define sharon stone\n",
        "sachin = embeddings[0]\n",
        "sachint= embeddings[1]\n",
        "# verify known photos of sharon\n",
        "print('Positive Tests')\n",
        "is_match(sachin, sachint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsFB7xiXVeBb"
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3QTwFZUVeAT"
      },
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}